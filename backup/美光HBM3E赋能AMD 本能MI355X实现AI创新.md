### 美光高带宽内存（HBM3E 36GB 12层）以及AMD Instinct™ MI350系列GPU和平台助力AI数据中心创新与发展。
**导语：2025年6月25日，美光发布了第三季度财报，HBM3E给股东带来了预期。我们来看下前两周发布HBM3E的新闻。**

美光的HBM3E 36GB 12层解决方案为AMD Instinct™ MI350系列GPU平台带来了行业领先的内存技术，提供出色的带宽和更低的功耗。

![Image](https://github.com/user-attachments/assets/b7512ca7-8faf-4d3e-aa26-ca84e30890eb)

基于AMD先进的CDNA 4架构打造的AMD Instinct MI350系列GPU平台集成了288GB高带宽HBM3E内存容量，可提供高达8TB/s的带宽，实现卓越的吞吐量。

![Image](https://github.com/user-attachments/assets/03762280-e5f5-4183-920f-de932567bff8)

爱达荷州博伊西市，2025年6月12日（环球新闻专线）——美光科技公司（纳斯达克股票代码：MU）今日宣布，其12层堆叠的36GB HBM3E产品将集成到即将推出的AMD Instinct™ MI350系列解决方案中。此次合作凸显了在训练大型人工智能模型、实现高吞吐量推理以及处理数据处理和计算建模等复杂高性能计算工作负载时，能效和性能的关键作用。此外，这也是美光在HBM行业领导地位的又一重要里程碑，展示了其强大的执行力以及牢固客户关系的价值。

美光12层堆叠的36GB HBM3E解决方案为AMD Instinct™ MI350系列GPU平台带来了行业领先的内存技术，提供出色的带宽和更低的功耗。1基于AMD先进的CDNA 4架构构建的AMD Instinct MI350系列GPU平台，集成了288GB的高带宽HBM3E内存容量，提供高达8TB/s的带宽，实现卓越的吞吐量。如此巨大的内存容量使Instinct MI350系列GPU能够在单个GPU上高效支持参数多达5200亿的人工智能模型。在完整的平台配置中，Instinct MI350系列GPU提供高达2.3TB的HBM3E内存，在FP4精度下达到高达161 PFLOPS的峰值理论性能，在高密度人工智能工作负载方面具备领先的能源效率和可扩展性。这种紧密集成的架构，结合美光高能效的HBM3E，为大语言模型训练、推理和科学模拟任务实现了卓越的吞吐量，使数据中心能够无缝扩展，同时最大限度地提高每瓦特的计算性能。美光与AMD的共同努力使人工智能解决方案能够更快地推向市场。 

“我们与AMD的紧密合作关系和联合工程努力，优化了美光12层堆叠的36GB HBM3E产品与Instinct MI350系列GPU和平台的兼容性。美光在HBM3E领域的行业领导地位和技术创新，为要求苛刻的人工智能系统提供高性能，为终端客户带来了更低的总体拥有成本（TCO）优势，”美光云内存产品副总裁兼总经理普拉文·瓦伊迪亚纳坦（Praveen Vaidyanathan）表示。

 “美光12层堆叠的36GB HBM3E产品对于释放AMD Instinct™ MI350系列加速器的性能和能源效率至关重要，”AMD Instinct产品工程公司副总裁乔希·弗里德里希（Josh Friedrich）表示。“我们与美光的持续合作推动了低功耗、高带宽内存的发展，帮助客户训练更大的人工智能模型、加快推理速度并应对复杂的高性能计算工作负载。” 美光12层堆叠的36GB HBM3E产品现已在多个领先的人工智能平台上通过认证。


可通过[原文](https://investors.micron.com/news-releases/news-release-details/micron-hbm-designed-leading-amd-ai-platform)了解更多资源：

- 12层堆叠的36GB HBM3E解决方案
- 生态系统合作伙伴计划：AMD 
- 人工智能数据中心 
- HBM3E图片库
